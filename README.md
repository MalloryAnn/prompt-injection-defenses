# prompt-injection-defenses

reference github: https://github.com/tldrsec/prompt-injection-defenses?tab=readme-ov-file

Notes:

** Starting with guardrails-ai/guardrails

-follow installation instrucitons

-test it/ run it

-start paper

Student Tasks

o Use provided datasets or small test cases.

**Body Paragraphs

Must show:

• What problem the tool solves

• How LLMs are used

• Input → processing → output

• At least one meaningful example

Walkthrough Report (4–6 pages, PDF)

**Body Paragraphs

Sections:

Project Overview

Security Problem Addressed

Architecture & Workflow

LLM Usage (model, prompts, fine-tuning, embeddings, etc.)

Strengths & Limitations

Reproducibility Notes
